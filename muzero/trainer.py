from __future__ import annotations

import os
import shutil
from pathlib import Path
from datetime import datetime
import numpy as np
import torch
from tqdm import trange
import polars as pl

from env.akioi_env import Akioi2048Env
from muzero.network import MuZeroNet
from muzero.mcts import Node, run_mcts, add_noise
from muzero.replay_buffer import Game, ReplayBuffer
from muzero.eval import eval

import printer

# åœ¨æ–‡ä»¶å¼€å¤´ import
import statistics


def play_one(env: Akioi2048Env, net: MuZeroNet, cfg: Config, device: str) -> Game:
    obs, _ = env.reset()
    g: Game = {
        "obs": [obs],
        "pi": [],
        "value": [],
        "reward": [],
        "total_reward": 0.0,
        "steps": 0,
    }

    root = Node(1.0)
    root.latent = net.representation(torch.tensor(obs[None], device=device))
    run_mcts(root, net, cfg, device)
    add_noise(root, cfg)

    done = False
    while not done:
        visits = np.array([c.visit for c in root.children.values()], np.float32)
        if visits.sum() == 0:
            visits = np.array([c.prior for c in root.children.values()], np.float32)
        visits /= visits.sum() + 1e-8
        action = int(np.random.choice(len(visits), p=visits))

        obs, r, done, *_ = env.step(action)
        g["pi"].append(visits)
        g["reward"].append(r)
        g["value"].append(root.value)
        g["total_reward"] += r
        g["steps"] += 1
        g["obs"].append(obs)

        child = root.children[action]
        if child.latent is None:
            a_one = torch.eye(cfg.action_space, device=device)[action].unsqueeze(0)
            child.latent, _ = net.dynamics(root.latent, a_one)
        root = child
        if not root.children:
            run_mcts(root, net, cfg, device)

    return g


def train(cfg: Config) -> None:
    last_loss = float("nan")
    # â¶ è®¾å¤‡é€‰æ‹©
    if torch.cuda.is_available():
        device = "cuda"
    elif torch.backends.mps.is_available():
        os.environ.setdefault("PYTORCH_ENABLE_MPS_FALLBACK", "1")
        device = "mps"
    else:
        device = "cpu"
    print("Running on", device)

    # â· ç»„ä»¶åˆå§‹åŒ–
    net = MuZeroNet(cfg).to(device)
    optim = torch.optim.Adam(net.parameters(), lr=cfg.lr_init)
    env = Akioi2048Env()
    rb = ReplayBuffer(cfg)
    ckdir = Path(cfg.checkpoint_dir)
    ckdir.mkdir(parents=True, exist_ok=True)

    # â¸ æ–­ç‚¹ç»­è®­
    start_g = 0

    resume_path = Path(f"{cfg.checkpoint_dir}/latest.pt")
    if resume_path.exists():
        ckpt = torch.load(resume_path, map_location=device)

        net.load_state_dict(ckpt["net"])
        optim.load_state_dict(ckpt["optim"])
        start_g = ckpt.get("game_idx", 0)
        print(f"âœ“ Resumed from {resume_path} (starting at game {start_g})")
        # restore replay buffer so batch_size is already met
        if "replay" in ckpt:
            rb.buf.extend(ckpt["replay"])
            print(f"âœ“ Restored {len(rb)} games into replay buffer")

    # â¹ è®­ç»ƒå¾ªç¯
    recent_scores, recent_steps = [], []
    for g_idx in trange(start_g, cfg.total_games, desc="games"):
        game = play_one(env, net, cfg, device)
        rb.add(game)
        recent_scores.append(game["total_reward"])
        recent_steps.append(game["steps"])
        if len(recent_scores) > cfg.log_interval:
            recent_scores.pop(0)
            recent_steps.pop(0)

        # ç½‘ç»œæ›´æ–°
        if len(rb) >= cfg.batch_size:
            for _ in range(cfg.update_per_game):
                obs, pi, v = rb.sample(device)
                h = net.representation(obs)
                logits, v_pred = net.prediction(h)
                loss = torch.nn.functional.cross_entropy(
                    logits, pi
                ) + torch.nn.functional.mse_loss(v_pred, v)
                last_loss = float(loss)
                optim.zero_grad()
                loss.backward()
                optim.step()

        # æ—¥å¿—
        if (g_idx + 1) % cfg.log_interval == 0:
            avg_s = np.mean(recent_scores) * 10_000
            avg_t = np.mean(recent_steps)
            loss_str = f"{last_loss:.4f}"
            print(
                f"[{g_idx + 1:>6}] buf={len(rb):6d}"
                f"  loss={loss_str:>6}"
                f"  avgScore={avg_s:8.2f}  avgSteps={avg_t:6.1f}"
            )

        # æµ‹è¯•
        # if (g_idx + 1) % cfg.eval_interval == 0:
        #     avg_eval = evaluate(net, env, cfg, device, n_games=20)
        #     print(f"ğŸ¯ [Eval @ {g_idx + 1}] avg-2048-score={avg_eval:.1f}")

        # ä¿å­˜ checkpoint â€”â€” å¸¦æ—¶é—´æˆ³ & æ›´æ–° latest.pt & test
        if cfg.save_interval and (g_idx + 1) % cfg.save_interval == 0:
            ts = datetime.now().strftime("%Y%m%d-%H%M%S")
            fname = f"muzero_{g_idx + 1:09d}_{ts}.pt"
            path = ckdir / fname
            torch.save(
                {
                    "net": net.state_dict(),
                    "optim": optim.state_dict(),
                    "cfg": cfg,
                    "game_idx": g_idx + 1,
                    "replay": list(rb.buf),
                },
                path,
            )
            print("âœ“ checkpoint saved â†’", path)

            # æ›´æ–° latest.pt æŒ‡å‘æœ€æ–°
            latest = ckdir / "latest.pt"
            try:
                if latest.exists() or latest.is_symlink():
                    latest.unlink()
                # åœ¨åŒä¸€ç›®å½•ä¸‹åˆ›å»ºç›¸å¯¹ç¬¦å·é“¾æ¥
                latest.symlink_to(fname)
            except (AttributeError, OSError):
                # æ— æ³•åˆ›å»º symlink æ—¶é€€å›åˆ°å¤åˆ¶
                shutil.copy(path, latest)
            print(f"âœ“ {latest} updated")
            if cfg.eval:
                eval_res = eval(cfg.eval_interval, f"{cfg.checkpoint_dir}/latest.pt")
                eval_special_res = [
                    min(eval_res),
                    max(eval_res),
                    sum(eval_res) / len(eval_res),
                ]
                eval_table = [["min", "max", "average"], eval_special_res]
                printer.print_table(eval_table)

                # ä¿å­˜è‡³csv
                csv_path = Path(cfg.csv_path)
                new_df = pl.DataFrame(
                    {
                        "id": [g_idx],
                        "min": [eval_special_res[0]],
                        "max": [eval_special_res[1]],
                        "average": [eval_special_res[2]],
                        "loss": [last_loss],
                    }
                )
                if csv_path.exists():
                    old_df = pl.read_csv(csv_path)
                    combined = pl.concat([old_df, new_df])
                else:
                    combined = new_df
                combined.write_csv(csv_path)
                print(f"âœ“ {csv_path} updated")


if __name__ == "__main__":
    # å…¨éƒ¨é…ç½®é›†ä¸­åœ¨è¿™é‡Œ
    from muzero.config import Config

    cfg = Config()
    train(cfg)
